seed: ${..seed}
algo: PPO # ExtrinsicAdapt, PPO
load_path: ${..checkpoint} # path to the checkpoint to load

network:
  mlp:
    units: [512, 256, 128]
  priv_mlp:
    units: [256, 128, 8]
  ft_mlp:
    units: [128, 64, 16]
  obs_mlp:
    units: [1024, 512, 128, 32]
  tactile_mlp:
    units: [300, 128, 32]
  merge_mlp:
    units: [128, 64, 4]
  contact_mlp:
    units: [ 128, 64, 16]

  tactile_decoder:
    num_channels: ${....task.tactile.decoder.num_channels}
    tactile_seq_length: ${....task.env.tactile_history_len}
    img_width: ${....task.tactile.decoder.width}
    img_height: ${....task.tactile.decoder.height}


ppo:
  output_name: 'debug'
  multi_gpu: False
  normalize_input: True
  normalize_value: True
  value_bootstrap: True
  num_actors: ${...task.env.numEnvs}
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  learning_rate: 5e-3
  kl_threshold: 0.02
  # PPO batch collection
  horizon_length: 24 # 64 , 8
  mini_epochs: 8 # 8
  minibatch_size: 24 # batch size = num_envs * horizon_length; minibatch_size = batch_size / num_minibatches

  # PPO loss setting
  clip_value: True
  critic_coef: 4
  entropy_coef: 0.0
  e_clip: 0.2
  bounds_loss_coef: 0.0001
  # grad clipping
  truncate_grads: True # False
  grad_norm: 1.0
  # snapshot setting
  save_best_after: 0
  save_frequency: 100
  max_agent_steps: 1500000000

  # rma setting
  priv_info: True
  extrin_adapt: False
  tactile_info: False
  ft_info: False
  obs_info: False
  ft_seq_length: ${...task.env.ft_history_len}
  obs_seq_length: ${...task.env.numObsHist}
  priv_info_dim: ${...task.env.numStates}
  compute_contact_gt: ${...task.env.compute_contact_gt}
  num_points: ${...task.env.num_points}

  student_obs_input_shape: ${...task.env.numObsStudent}
  save_buffer: False
  ft_input_dim: 6